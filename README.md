# HugginfFaceLMTutorial
Huggingface의 transformers &amp; tokenizers를 활용한 다양한 LM의 pretraining &amp; finetuning tutorial 코드


- Data for pretraining   
* 한국어 위키피디아   

- Data for finetuning   
* 단일 문장 분류   
* 두 문장 관계 분류   
* 문장 토큰 분류   

- Pretraining models   
   
1. BERT  
2. RoBERTa   
3. GPT-2   
4. BART   
